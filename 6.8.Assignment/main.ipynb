{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ICsneQMSC4AV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TvGNFfQZN4nc",
    "outputId": "7fbfbcb6-7864-4351-c5ad-64001aaed319"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 1797\\n:Number of Attributes: 64\\n:Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n:Missing Attribute Values: None\\n:Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n:Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. dropdown:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_digits()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QqTHqvzORkV",
    "outputId": "dd17cff1-40e1-4e54-fd0f-9e1ad0834714"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArXN-1BXO1Ii",
    "outputId": "eadcf7fe-47f7-4737-95c2-84bb91e62106"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtwIuXJgPBB2",
    "outputId": "a6a9ef05-7c96-4fe8-a39d-345bc64f8f03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nNQK4IFQQAAv"
   },
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# one-hot\n",
    "y = np.eye(10)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yjysf1WKQbWr",
    "outputId": "cacbcc69-bb48-4a4f-e150-9198389fc945"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64), (1437, 10), (360, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6WiezzAsQbaS"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "\n",
    "def cross_entropy_error(Y_pred, Y_gt):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(Y_gt * np.log(Y_pred + delta))\n",
    "\n",
    "\n",
    "def root_mean_squired_error(Y_pred, Y_gt):\n",
    "    return np.sqrt(np.mean((Y_pred - Y_gt) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "d7M-plZ_SeAW"
   },
   "outputs": [],
   "source": [
    "epochs = 80\n",
    "lr = 0.001\n",
    "\n",
    "D_in = X_train.shape[1]\n",
    "H1_layer = 128\n",
    "H2_layer = 32\n",
    "# D_out = len(np.unique(y_train))\n",
    "D_out = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MEJXPwMqZPp0"
   },
   "outputs": [],
   "source": [
    "W1 = np.random.randn(D_in, H1_layer)\n",
    "W2 = np.random.randn(H1_layer, H2_layer)\n",
    "W3 = np.random.randn(H2_layer, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "993Efi_fZpwv"
   },
   "outputs": [],
   "source": [
    "b1 = np.random.randn(1, H1_layer)\n",
    "b2 = np.random.randn(1, H2_layer)\n",
    "b3 = np.random.randn(1, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQ6CJYakaBnr",
    "outputId": "93b679c7-281e-432c-e8b6-cf47b30dc0c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train: 0.3267476797305985 accuracy train: 0.14335421016005567\n",
      "loss test: 0.30643636749018344 accuracy test: 0.18888888888888888\n",
      "loss train: 0.29107603787540515 accuracy train: 0.2755741127348643\n",
      "loss test: 0.28272570207302755 accuracy test: 0.34444444444444444\n",
      "loss train: 0.26813765797568906 accuracy train: 0.43075852470424497\n",
      "loss test: 0.26805103589837087 accuracy test: 0.41388888888888886\n",
      "loss train: 0.2535795475806087 accuracy train: 0.5337508698677801\n",
      "loss test: 0.25818283051880875 accuracy test: 0.49166666666666664\n",
      "loss train: 0.2422366093569188 accuracy train: 0.5845511482254697\n",
      "loss test: 0.24909336172838795 accuracy test: 0.5361111111111111\n",
      "loss train: 0.23200894232562289 accuracy train: 0.6318719554627696\n",
      "loss test: 0.24157926011824918 accuracy test: 0.5833333333333334\n",
      "loss train: 0.22228775628394223 accuracy train: 0.6694502435629784\n",
      "loss test: 0.23561516032508567 accuracy test: 0.6166666666666667\n",
      "loss train: 0.21369206967523385 accuracy train: 0.7132915796798887\n",
      "loss test: 0.23009706578523595 accuracy test: 0.6388888888888888\n",
      "loss train: 0.20608188347368775 accuracy train: 0.7397355601948504\n",
      "loss test: 0.22515183134252034 accuracy test: 0.6472222222222223\n",
      "loss train: 0.19928969349996625 accuracy train: 0.7620041753653445\n",
      "loss test: 0.22056491027733244 accuracy test: 0.6555555555555556\n",
      "loss train: 0.19320563581797529 accuracy train: 0.7773138482950591\n",
      "loss test: 0.21660290215220981 accuracy test: 0.6611111111111111\n",
      "loss train: 0.1876908660490176 accuracy train: 0.791231732776618\n",
      "loss test: 0.21286857660780853 accuracy test: 0.6694444444444444\n",
      "loss train: 0.1825542737434597 accuracy train: 0.8100208768267223\n",
      "loss test: 0.20923660101134284 accuracy test: 0.6833333333333333\n",
      "loss train: 0.1776530650139722 accuracy train: 0.8190675017397355\n",
      "loss test: 0.20554118665654045 accuracy test: 0.7027777777777777\n",
      "loss train: 0.17299685672411938 accuracy train: 0.8274182324286709\n",
      "loss test: 0.20240138298105814 accuracy test: 0.7138888888888889\n",
      "loss train: 0.16857583135435228 accuracy train: 0.8364648573416841\n",
      "loss test: 0.19917977454389454 accuracy test: 0.7305555555555555\n",
      "loss train: 0.16422050577568884 accuracy train: 0.8448155880306193\n",
      "loss test: 0.19592629245461113 accuracy test: 0.7333333333333333\n",
      "loss train: 0.16005860326490576 accuracy train: 0.8552540013917884\n",
      "loss test: 0.1925731217021685 accuracy test: 0.7361111111111112\n",
      "loss train: 0.1560387559904028 accuracy train: 0.8684759916492694\n",
      "loss test: 0.18969287578963984 accuracy test: 0.7527777777777778\n",
      "loss train: 0.1523404578048737 accuracy train: 0.8747390396659708\n",
      "loss test: 0.18709661857683568 accuracy test: 0.7638888888888888\n",
      "loss train: 0.14876818664771133 accuracy train: 0.8803061934585943\n",
      "loss test: 0.184746660879463 accuracy test: 0.7694444444444445\n",
      "loss train: 0.14555044529485525 accuracy train: 0.8907446068197634\n",
      "loss test: 0.18251837811043364 accuracy test: 0.7805555555555556\n",
      "loss train: 0.14257269357277358 accuracy train: 0.8990953375086986\n",
      "loss test: 0.18040191354545926 accuracy test: 0.7861111111111111\n",
      "loss train: 0.13977072566060414 accuracy train: 0.9039665970772442\n",
      "loss test: 0.1783987323976723 accuracy test: 0.7944444444444444\n",
      "loss train: 0.13708804133976826 accuracy train: 0.9067501739735561\n",
      "loss test: 0.1764736636906764 accuracy test: 0.8\n",
      "loss train: 0.13446755238975872 accuracy train: 0.9116214335421016\n",
      "loss test: 0.17454288469299237 accuracy test: 0.7972222222222223\n",
      "loss train: 0.13192053702393133 accuracy train: 0.9157967988865693\n",
      "loss test: 0.17276134729414067 accuracy test: 0.8055555555555556\n",
      "loss train: 0.1294480032229905 accuracy train: 0.9178844815588031\n",
      "loss test: 0.17105966583950025 accuracy test: 0.8055555555555556\n",
      "loss train: 0.12700541294762316 accuracy train: 0.9220598469032707\n",
      "loss test: 0.16937296887130499 accuracy test: 0.8111111111111111\n",
      "loss train: 0.12464116266371814 accuracy train: 0.9255393180236604\n",
      "loss test: 0.1677974950356563 accuracy test: 0.8111111111111111\n",
      "loss train: 0.1223726081193636 accuracy train: 0.929714683368128\n",
      "loss test: 0.16634217466981988 accuracy test: 0.8166666666666667\n",
      "loss train: 0.12015847845458569 accuracy train: 0.930410577592206\n",
      "loss test: 0.16502610721690955 accuracy test: 0.8194444444444444\n",
      "loss train: 0.11798227457596738 accuracy train: 0.9318023660403618\n",
      "loss test: 0.16371790599792546 accuracy test: 0.8222222222222222\n",
      "loss train: 0.1159690071228636 accuracy train: 0.9338900487125957\n",
      "loss test: 0.1623490645138714 accuracy test: 0.8222222222222222\n",
      "loss train: 0.11403545555574796 accuracy train: 0.9387613082811412\n",
      "loss test: 0.16093918613025357 accuracy test: 0.8277777777777777\n",
      "loss train: 0.11215322549623134 accuracy train: 0.9394572025052192\n",
      "loss test: 0.15956007970894007 accuracy test: 0.8305555555555556\n",
      "loss train: 0.11026674596731145 accuracy train: 0.9436325678496869\n",
      "loss test: 0.1581635075410704 accuracy test: 0.8333333333333334\n",
      "loss train: 0.10839903982908733 accuracy train: 0.9450243562978428\n",
      "loss test: 0.1567590938737701 accuracy test: 0.8388888888888889\n",
      "loss train: 0.10656811978104887 accuracy train: 0.9498956158663883\n",
      "loss test: 0.15541338320936077 accuracy test: 0.8472222222222222\n",
      "loss train: 0.104782480874704 accuracy train: 0.9519832985386222\n",
      "loss test: 0.15415835535203368 accuracy test: 0.8472222222222222\n",
      "loss train: 0.10304191307976505 accuracy train: 0.954070981210856\n",
      "loss test: 0.15299596548823965 accuracy test: 0.8444444444444444\n",
      "loss train: 0.10140575260634249 accuracy train: 0.9547668754349339\n",
      "loss test: 0.15192474554837992 accuracy test: 0.85\n",
      "loss train: 0.09986714171747726 accuracy train: 0.9554627696590118\n",
      "loss test: 0.15093563725414366 accuracy test: 0.8527777777777777\n",
      "loss train: 0.09838610446632158 accuracy train: 0.9568545581071677\n",
      "loss test: 0.15001098788849318 accuracy test: 0.8527777777777777\n",
      "loss train: 0.09693690311786601 accuracy train: 0.9589422407794015\n",
      "loss test: 0.1491148798356592 accuracy test: 0.8527777777777777\n",
      "loss train: 0.09555144605360288 accuracy train: 0.9596381350034795\n",
      "loss test: 0.1482557962617192 accuracy test: 0.8555555555555555\n",
      "loss train: 0.09423574870797309 accuracy train: 0.9610299234516354\n",
      "loss test: 0.14744995131536628 accuracy test: 0.8611111111111112\n",
      "loss train: 0.09297483795419201 accuracy train: 0.9617258176757133\n",
      "loss test: 0.14670285084024762 accuracy test: 0.8611111111111112\n",
      "loss train: 0.09176136925770827 accuracy train: 0.9617258176757133\n",
      "loss test: 0.14601562294329906 accuracy test: 0.8666666666666667\n",
      "loss train: 0.09059039554947403 accuracy train: 0.9624217118997912\n",
      "loss test: 0.14538084849788008 accuracy test: 0.8666666666666667\n",
      "loss train: 0.08945940042149343 accuracy train: 0.9631176061238692\n",
      "loss test: 0.1447916274560469 accuracy test: 0.8666666666666667\n",
      "loss train: 0.08835811506564062 accuracy train: 0.965205288796103\n",
      "loss test: 0.14424458606157622 accuracy test: 0.8666666666666667\n",
      "loss train: 0.08716307197918384 accuracy train: 0.9665970772442589\n",
      "loss test: 0.14370856119563355 accuracy test: 0.8666666666666667\n",
      "loss train: 0.0860377177239617 accuracy train: 0.9665970772442589\n",
      "loss test: 0.14318332289189215 accuracy test: 0.8694444444444445\n",
      "loss train: 0.08497137215799015 accuracy train: 0.9665970772442589\n",
      "loss test: 0.1426707756001403 accuracy test: 0.8722222222222222\n",
      "loss train: 0.08391606778896558 accuracy train: 0.9665970772442589\n",
      "loss test: 0.14216688211206588 accuracy test: 0.8722222222222222\n",
      "loss train: 0.08289424288370174 accuracy train: 0.9679888656924147\n",
      "loss test: 0.14168381409360914 accuracy test: 0.875\n",
      "loss train: 0.08191180296168678 accuracy train: 0.9679888656924147\n",
      "loss test: 0.14122864375580446 accuracy test: 0.875\n",
      "loss train: 0.08096322159917388 accuracy train: 0.9693806541405706\n",
      "loss test: 0.14080054620893348 accuracy test: 0.875\n",
      "loss train: 0.08004396606811313 accuracy train: 0.9693806541405706\n",
      "loss test: 0.14040263345886428 accuracy test: 0.875\n",
      "loss train: 0.07914751758290095 accuracy train: 0.9700765483646486\n",
      "loss test: 0.14003960138507637 accuracy test: 0.875\n",
      "loss train: 0.07827077024658383 accuracy train: 0.9700765483646486\n",
      "loss test: 0.1397108591420674 accuracy test: 0.875\n",
      "loss train: 0.07741649097631727 accuracy train: 0.9707724425887265\n",
      "loss test: 0.13940926157660125 accuracy test: 0.875\n",
      "loss train: 0.07658284567369694 accuracy train: 0.9707724425887265\n",
      "loss test: 0.1391268893207884 accuracy test: 0.875\n",
      "loss train: 0.07576271910849892 accuracy train: 0.9707724425887265\n",
      "loss test: 0.1388589292503557 accuracy test: 0.875\n",
      "loss train: 0.07494882884857812 accuracy train: 0.9707724425887265\n",
      "loss test: 0.13860500620847357 accuracy test: 0.875\n",
      "loss train: 0.07413795768939958 accuracy train: 0.9714683368128044\n",
      "loss test: 0.13836245022225233 accuracy test: 0.8694444444444445\n",
      "loss train: 0.07333024845054502 accuracy train: 0.9714683368128044\n",
      "loss test: 0.1381224231860507 accuracy test: 0.8694444444444445\n",
      "loss train: 0.07251478111778709 accuracy train: 0.9714683368128044\n",
      "loss test: 0.13789175488100378 accuracy test: 0.8666666666666667\n",
      "loss train: 0.07159443843427611 accuracy train: 0.9714683368128044\n",
      "loss test: 0.13784928612100547 accuracy test: 0.8666666666666667\n",
      "loss train: 0.07069785047104338 accuracy train: 0.9721642310368824\n",
      "loss test: 0.13769981652017554 accuracy test: 0.8666666666666667\n",
      "loss train: 0.06990657829931263 accuracy train: 0.9728601252609603\n",
      "loss test: 0.13751786053443799 accuracy test: 0.8694444444444445\n",
      "loss train: 0.06912725257255714 accuracy train: 0.9735560194850382\n",
      "loss test: 0.1372843491371792 accuracy test: 0.8722222222222222\n",
      "loss train: 0.06837198437788908 accuracy train: 0.9735560194850382\n",
      "loss test: 0.13700700021119716 accuracy test: 0.8722222222222222\n",
      "loss train: 0.06765537870986497 accuracy train: 0.9742519137091162\n",
      "loss test: 0.13673602877771754 accuracy test: 0.8722222222222222\n",
      "loss train: 0.06697396409566274 accuracy train: 0.9742519137091162\n",
      "loss test: 0.13648523814646848 accuracy test: 0.8722222222222222\n",
      "loss train: 0.06631963310729583 accuracy train: 0.9749478079331941\n",
      "loss test: 0.13624559812789408 accuracy test: 0.8694444444444445\n",
      "loss train: 0.0656858361906992 accuracy train: 0.9749478079331941\n",
      "loss test: 0.1360107741035586 accuracy test: 0.8694444444444445\n",
      "loss train: 0.0650684159019949 accuracy train: 0.9749478079331941\n",
      "loss test: 0.13578028039843276 accuracy test: 0.8694444444444445\n",
      "loss train: 0.06446367224687248 accuracy train: 0.9749478079331941\n",
      "loss test: 0.13555545211450953 accuracy test: 0.8694444444444445\n",
      "train completed!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    # train\n",
    "    Y_pred_train = []\n",
    "    for x, y in zip(X_train, y_train):\n",
    "\n",
    "        # forward\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # layer 1\n",
    "        net1 = x.T @ W1 + b1\n",
    "        out1 = sigmoid(net1)\n",
    "\n",
    "        # layer 2\n",
    "        net2 = out1 @ W2 + b2\n",
    "        out2 = sigmoid(net2)\n",
    "\n",
    "        # layer 3\n",
    "        net3 = out2 @ W3 + b3\n",
    "        out3 = softmax(net3)\n",
    "\n",
    "        y_pred = out3\n",
    "        Y_pred_train.append(y_pred.T)\n",
    "\n",
    "        # back propagation\n",
    "        # layer 3\n",
    "        error = -2 * (y - y_pred)\n",
    "        grad_W3 = out2.T @ error\n",
    "        grad_b3 = error\n",
    "\n",
    "        # layer 2\n",
    "        error = error @ W3.T * out2 * (1 - out2)\n",
    "        grad_W2 = out1.T @ error\n",
    "        grad_b2 = error\n",
    "\n",
    "        # layer 1\n",
    "        error = error @ W2.T * out1 * (1 - out1)\n",
    "        grad_W1 = x @ error\n",
    "        grad_b1 = error\n",
    "\n",
    "        # update\n",
    "        # layer 1\n",
    "        W1 = W1 - lr * grad_W1\n",
    "        b1 = b1 - lr * grad_b1\n",
    "\n",
    "        # layer 2\n",
    "        W2 = W2 - lr * grad_W2\n",
    "        b2 = b2 - lr * grad_b2\n",
    "\n",
    "        # layer 3\n",
    "        W3 = W3 - lr * grad_W3\n",
    "        b3 = b3 - lr * grad_b3\n",
    "\n",
    "    Y_pred_train = np.array(Y_pred_train).reshape(-1, 10)\n",
    "    loss_train = root_mean_squired_error(Y_pred_train, y_train)\n",
    "    acc_train = np.mean(np.argmax(Y_pred_train, axis=1) == np.argmax(y_train, axis=1))\n",
    "\n",
    "    # test\n",
    "    Y_pred_test = []\n",
    "    for x, y in zip(X_test, y_test):\n",
    "\n",
    "        # forward\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # layer 1\n",
    "        net1 = x.T @ W1 + b1\n",
    "        out1 = sigmoid(net1)\n",
    "\n",
    "        # layer 2\n",
    "        net2 = out1 @ W2 + b2\n",
    "        out2 = sigmoid(net2)\n",
    "\n",
    "        # layer 3\n",
    "        net3 = out2 @ W3 + b3\n",
    "        out3 = softmax(net3)\n",
    "\n",
    "        y_pred = out3\n",
    "        Y_pred_test.append(y_pred.T)\n",
    "\n",
    "    Y_pred_test = np.array(Y_pred_test).reshape(-1, 10)\n",
    "    loss_test = root_mean_squired_error(Y_pred_test, y_test)\n",
    "    acc_test = np.mean(np.argmax(Y_pred_test, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "    print('loss train:', loss_train, 'accuracy train:', acc_train)\n",
    "    print('loss test:', loss_test, 'accuracy test:', acc_test)\n",
    "\n",
    "print('train completed!')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
